# KnowledgeSharing-Pytorch
This repository is maintained to implement some state-of-the-art knowledge distillation and knowledge transfer methods.

## ToDo List


## Knowledge Distillation (KD)
Knowledge distillation was proposed to distill knowledge from a large teacher network to a smaller student network. KD can help the student model to achieve higher generalization performance. It's applications include model compression.

## Knowledge Transfer (KT)


 
### Model List
- Basic knowledge distillation
- Born-again Neural Networks
- Knowledge Transfer with Jacobian Matching
- Deep Mutual Learning
- Co-teaching
- One-the-fly Native Ensemble
- MentorNet


